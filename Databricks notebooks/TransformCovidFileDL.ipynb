{"cells":[{"cell_type":"markdown","source":["##Adding a new Mount point to Azure Data Lake Gen2\n\nSource: [Access Azure Data Lake Storage Gen2 using OAuth 2.0 with an Azure service principal](https://docs.databricks.com/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access.html)\n\nWe need a few bits of information to create a mount point. We nedd:\n* The **Service Principal ID** - (Azure Portal -> Azure Active Directory service -> App Registrations -> Application (client) ID value)\n* The **Service Principal Key** - (Azure Portal -> Azure Active Directory service -> App Registrations -> Certificates & Secrets -> Client secrets value)\n* The **DirectoryID** - (Azure Portal -> Azure Active Directory service -> App Registrations -> Directory (tenant) ID value)\n* The ADLS storage account name\n* The ADLS container name\n\n**Important.** To get Service Principal ID, Service Principal Key, DirectoryID you need create an Azure AD application, which will create an associated service principal used to access the storage account:\n1. On Azure portal go to the Azure Active Directory service\n2. Under Manage, click App Registrations.\n3. Click + New registration. Enter a name for the application and click Register."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Addind a new Mount point to Azure Data Lake Gen2","showTitle":false,"inputWidgets":{},"nuid":"d2c07e6c-299a-4fa9-899e-18dbfe8a2954"}}},{"cell_type":"markdown","source":["###Adding scoped secrets\n\nTo add a secret and a scope, this needs to be completed using the Databricks CLI.\n\nIf you need the Databricks CLI, you can pip install it localy: ```pip install databricks-cli```\nAlso, you can use **Cloud Shell** for this purpose.\n\nYou need to have created a personal access token (PAT) prior to using the CLI. (Databricks - User Settings - Generate New Token)\n\nUse the command: ```databricks configure --token``` to configure the Databricks CLI\n\nOnce you're connected it is easy as this:\n1. ```databricks secrets create-scope --scope Analysts --initial-manage-principal \"users\"```\n2. ```databricks secrets put --scope Analysts --key SPID --string-value \"Service Principal ID\"```\n3. ```databricks secrets put --scope Analysts --key SPKey --string-value \"Service Principal Key\"```\n4. ```databricks secrets put --scope Analysts --key DirectoryID --string-value \"Azure Directory ID\"```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa94f2c2-8cb4-498c-91a1-f2257da52116"}}},{"cell_type":"markdown","source":["The **NOT** recommended approach:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b11f3b85-886f-4653-84ad-9af70ff853c0"}}},{"cell_type":"code","source":["ServicePrincipalID = \"<Service Principal ID>\"\nServicePrincipalKey = \"<Service Principal Secret Key>\"\nDirectoryID = \"Azure Directory ID\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65af5952-7ad9-435a-97a6-550ba8fd17ce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The recommended approach:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e606bfdd-e790-4388-a1d8-858185dced04"}}},{"cell_type":"code","source":["import itertools as it\nimport sys\nimport os\n\n# Gather relevant keys\nServicePrincipalID = dbutils.secrets.get(scope = 'Analysts', key = 'SPID')\nServicePrincipalKey = dbutils.secrets.get(scope = 'Analysts', key = 'SPKey')\nDirectoryID = dbutils.secrets.get(scope = 'Analysts', key = 'DirectoryID')\n\n# Combine DirectoryID into full string\nDirectory = f\"https://login.microsoftonline.com/{DirectoryID}/oauth2/token\"\n\n# Input parameter: file for transform\ndbutils.widgets.text(\"fileName\", \"\", \"\")\ninput_file_name = \"CoordStatusMA_Short.csv\"\n\n# Storage account options\n# ADLS container name\ncontainer_name = \"covidsbcontainer\"\n# ADLS storage account name\nstac_name = \"covidsbstac2\"\n\n# Variables for DBFS \nurl = f\"abfss://{container_name}@{stac_name}.dfs.core.windows.net/\"\nmnt_path = '/mnt/covid'\ninput_file_path = \"/dbfs\" + mnt_path + \"/input/\" + input_file_name\noutput_file_path = \"/dbfs\" + mnt_path + \"/output/\" + os.path.splitext(os.path.basename(input_file_path))[0] + \"_prepared.csv\"\n\n# Create configurations for our connection\nconfigs = {\"fs.azure.account.auth.type\": \"OAuth\",\n           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n           \"fs.azure.account.oauth2.client.id\": ServicePrincipalID,\n           \"fs.azure.account.oauth2.client.secret\": ServicePrincipalKey,\n           \"fs.azure.account.oauth2.client.endpoint\": Directory}\n\n# Mount the Data Lake onto DBFS at the /mnt/covid location\ndbutils.fs.mount(\n  source = url,\n  mount_point = mnt_path,\n  extra_configs = configs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f8d9115-3be3-4998-bc90-d84ca0bbf031"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: True</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["status = None\ndate = None\ncolumn_names=['Status','Date','Latitude','Longitude','Count']\nif os.path.isfile(input_file_path):\n  new_file = open(output_file_path, 'w')\n  new_file.write(column_names[0] + \",\" + column_names[1] + \",\" + column_names[2] + \",\" + column_names[3] + \",\" + column_names[4] +\"\\n\")\n  with open(input_file_path, mode='r', encoding='cp1252') as f:\n      for line in f.readlines():\n          if len(line.strip()) > 0:\n              if line.startswith('Status='):\n                  status_line = line.split(',')[0]\n                  status = status_line.split('=')[1]\n              else:\n                  #Get date\n                  date = line.split(',')[0]\n                  #Get list with Latitudes\n                  _la = line.split(',')[1::3]\n                  #Get list with Longitudes\n                  _lo = line.split(',')[2::3]\n                  #Get list with counts\n                  _c = line.split(',')[3::3]\n                  if len(_la) == len(_lo) == len(_c):\n                      #Aggregate elements from lists\n                      for (la, lo, c) in zip(_la, _lo, _c):\n                          #while lists are not empty\n                          if len(la) > 0 and len(lo) > 0 and len(c) > 0:\n                              new_file.write(status.strip() + \",\" + date.strip() + \",\" + la.strip() + \",\" + lo.strip() + \",\" + c.strip() +\"\\n\")\n  new_file.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3b78161-db2f-4a68-bbb6-5e7e39a774a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Unmount the data lake\ndbutils.fs.unmount(mnt_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbe1adde-9d2d-4006-b0db-fd67bb8fb847"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/covid has been unmounted.\nOut[10]: True</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/covid has been unmounted.\nOut[10]: True</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"TransformCovidFileDL","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"fileName":{"nuid":"4fa638d4-db23-4d66-8b6f-18b786c92c94","currentValue":"","widgetInfo":{"widgetType":"text","name":"fileName","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":527959288423596}},"nbformat":4,"nbformat_minor":0}
